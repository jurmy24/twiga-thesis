{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study Calculations\n",
    "This notebook contains all my code to run the ablation study and to compute the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "# from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "import time\n",
    "\n",
    "# load_dotenv()\n",
    "# DATA_DIR = os.getenv(\"DATA_DIR_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.getcwd() # Get the current working directory of the notebook\n",
    "src_dir = os.path.abspath(os.path.join(notebook_dir, '..', '..')) # Construct the path to the src directory\n",
    "sys.path.append(src_dir) # Add the src directory to the system path\n",
    "\n",
    "from src.pipelines.pipeline_runner import run_data_through_generator\n",
    "from src.utils import load_json_to_pipelinedata, save_objects_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the ablation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 15138459: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     incomplete_pipeline_data_control \u001b[38;5;241m=\u001b[39m load_json_to_pipelinedata(data_control)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(input_file_main, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 60\u001b[0m     data_main \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     incomplete_pipeline_data_main \u001b[38;5;241m=\u001b[39m load_json_to_pipelinedata(data_main)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# # Run the ablation study using llama3-70b-8192 due to its success in the \u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 15138459: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "what_to_ablate = Literal[\"query-rewriter\", \"sparse-retriever\", \"reranker\", \"sample-questions\", \"one-shot\"] \n",
    "\n",
    "################################\n",
    "# What do you wish to get rid of in this run?\n",
    "what_to_ablate = \"query-rewriter\"\n",
    "################################\n",
    "\n",
    "for what_to_ablate in [\"sparse-retriever\", \"reranker\", \"sample-questions\", \"one-shot\"]:\n",
    "\n",
    "    ablation_params = {\n",
    "            \"no_rewriter\": False,\n",
    "            \"no_sparse_retriever\": False,\n",
    "            \"no_reranker\": False,\n",
    "            \"no_sample_questions\": False,\n",
    "            \"no_one_shot\": False\n",
    "        }\n",
    "\n",
    "    if what_to_ablate == \"query-rewriter\":\n",
    "        input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-query-rewrite).json\")\n",
    "        output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite.json\")\n",
    "\n",
    "        input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-query-rewrite).json\")\n",
    "        output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite-control.json\")\n",
    "        ablation_params[\"no_rewriter\"] = True\n",
    "    elif what_to_ablate == \"sparse-retriever\":\n",
    "        input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-sparse-retrieval).json\")\n",
    "        output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever.json\")\n",
    "\n",
    "        input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-sparse-retrieval).json\")\n",
    "        output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever-control.json\")\n",
    "        ablation_params[\"no_sparse_retriever\"] = True\n",
    "    elif what_to_ablate == \"reranker\":\n",
    "        input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-query-rewrite).json\")\n",
    "        output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker.json\")\n",
    "\n",
    "        input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-query-rewrite).json\")\n",
    "        output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker-control.json\")\n",
    "        ablation_params[\"no_reranker\"] = True\n",
    "    elif what_to_ablate == \"sample-questions\":\n",
    "        input_file_main = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"test-prompts-rewritten-retrieved.json\")\n",
    "        output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions.json\")\n",
    "        \n",
    "        input_file_control = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"control-test-prompts-rewritten-retrieved.json\")\n",
    "        output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions-control.json\")\n",
    "        ablation_params[\"no_sample_questions\"] = True\n",
    "    elif what_to_ablate == \"one-shot\":\n",
    "        input_file_main = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"test-prompts-rewritten-retrieved.json\")\n",
    "        output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot.json\")\n",
    "        input_file_control = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"control-test-prompts-rewritten-retrieved.json\")\n",
    "        output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot-control.json\")\n",
    "        ablation_params[\"no_one_shot\"] = True\n",
    "    else: \n",
    "        raise Exception(\"You haven't specified a correct ablation to generate.\")\n",
    "\n",
    "    with open(input_file_control, 'r', encoding=\"utf-8\") as file:\n",
    "        data_control = json.load(file)\n",
    "        incomplete_pipeline_data_control = load_json_to_pipelinedata(data_control)\n",
    "\n",
    "    with open(input_file_main, 'r', encoding=\"utf-8\") as file:\n",
    "        data_main = json.load(file)\n",
    "        incomplete_pipeline_data_main = load_json_to_pipelinedata(data_main)\n",
    "\n",
    "    # # Run the ablation study using llama3-70b-8192 due to its success in the \n",
    "    res = run_data_through_generator(incomplete_pipeline_data_control, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\n",
    "    save_objects_as_json(res, output_file_control, rewrite=True)\n",
    "\n",
    "    def process_in_batches(data, batch_size, wait_time, output_file, model, ablation_params, verbose=False):\n",
    "        total_elements = len(data)\n",
    "        for i in range(0, total_elements, batch_size):\n",
    "\n",
    "            batch = data[i:i + batch_size]\n",
    "            res = run_data_through_generator(batch, model, ablation_params=ablation_params, verbose=verbose)\n",
    "            save_objects_as_json(res, output_file, rewrite=False)\n",
    "            if i + batch_size < total_elements:\n",
    "                print(\"Sleepy time...\")\n",
    "                time.sleep(wait_time)\n",
    "                print(\"Okay I'm back now...\")\n",
    "                print(f\"i={i}\")\n",
    "\n",
    "    process_in_batches(incomplete_pipeline_data_main, 30, 5, output_file_main, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study automatic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data from pipeline...: 100%|██████████| 30/30 [00:20<00:00,  1.47it/s]\n",
      "Processing data from pipeline...: 100%|██████████| 300/300 [03:14<00:00,  1.54it/s]\n",
      "Generating cosine similarities...: 100%|██████████| 30/30 [00:00<00:00, 4128.12it/s]\n",
      "Generating cosine similarities...: 100%|██████████| 300/300 [00:00<00:00, 5549.20it/s]\n",
      "Computing K-F1++ scores: 100%|██████████| 30/30 [00:00<00:00, 487.26it/s]\n",
      "Computing K-F1++ scores: 100%|██████████| 300/300 [00:00<00:00, 729.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from evals.automatic.test_utils import extract_eval_data\n",
    "from evals.automatic.hit_ratio_and_mrr import compute_hit_ratio_and_mrr\n",
    "from evals.automatic.BERTscore import bertscore_computation_pipeline\n",
    "from evals.automatic.test_utils import append_to_file\n",
    "from evals.automatic.embedding_similarity import cosine_similarity_pipeline_from_stored_embeddings\n",
    "from evals.automatic.k_f1_plus_plus import compute_avg_kf1_score\n",
    "from bert_score import BERTScorer\n",
    "\n",
    "################################\n",
    "what_to_ablate = Literal[\"query-rewriter\", \"sparse-retriever\", \"reranker\", \"sample-questions\", \"one-shot\"] \n",
    "\n",
    "# What do you wish to exclude?\n",
    "what_to_ablate = \"one-shot\"\n",
    "################################\n",
    "\n",
    "# This is where we will store all results\n",
    "results_file = os.path.join(\"..\", \"..\", \"evals\", \"ablation\", \"results\", \"results.txt\")\n",
    "\n",
    "if what_to_ablate == \"query-rewriter\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite-control.json\")\n",
    "elif what_to_ablate == \"sparse-retriever\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever-control.json\")\n",
    "elif what_to_ablate == \"reranker\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker-control.json\")\n",
    "elif what_to_ablate == \"sample-questions\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions-control.json\")\n",
    "elif what_to_ablate == \"one-shot\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot-control.json\")\n",
    "else: \n",
    "    raise Exception(\"You haven't specified a correct ablation to evaluate.\")\n",
    "\n",
    "# Extract the data to PipelineData format\n",
    "pipeline_data_main = extract_eval_data(data_file_main)\n",
    "pipeline_data_control = extract_eval_data(data_file_control)\n",
    "\n",
    "\"\"\"Compute the Hit Rate and MRR\"\"\"\n",
    "if what_to_ablate not in [\"sample-questions\", \"one-shot\"]:\n",
    "    # For control\n",
    "    _, hit_rate, mrr = compute_hit_ratio_and_mrr(pipeline_data_control)\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} Hit Rate: {hit_rate}\")\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} MRR: {mrr}\")\n",
    "\n",
    "    # For main\n",
    "    _, hit_rate, mrr = compute_hit_ratio_and_mrr(pipeline_data_main)\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} Hit Rate: {hit_rate}\")\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} MRR: {mrr}\")\n",
    "\n",
    "\"\"\"Compute BERTScore\"\"\"\n",
    "# BERTScore scorer\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "_, P_content, R_content, F1_content, _, _, _ = bertscore_computation_pipeline(pipeline_data_control, scorer)\n",
    "append_to_file(results_file, f\"CONTROL-{what_to_ablate} BERTscore Precision: {P_content}, Recall: {R_content}, F1: {F1_content}\")\n",
    "_, P_content, R_content, F1_content, _, _, _ = bertscore_computation_pipeline(pipeline_data_main, scorer)\n",
    "append_to_file(results_file, f\"MAIN-{what_to_ablate} BERTscore Precision: {P_content}, Recall: {R_content}, F1: {F1_content}\")\n",
    "\n",
    "\"\"\"Compute Embedding Similarity\"\"\"\n",
    "_, similarity_content, _ = cosine_similarity_pipeline_from_stored_embeddings(pipeline_data_control)\n",
    "append_to_file(results_file, f\"CONTROL-{what_to_ablate} all-MiniLM-l6-v2 Cosine Similarity: {similarity_content}\")\n",
    "_, similarity_content, _ = cosine_similarity_pipeline_from_stored_embeddings(pipeline_data_main)\n",
    "append_to_file(results_file, f\"MAIN-{what_to_ablate} all-MiniLM-l6-v2 Cosine Similarity: {similarity_content}\")\n",
    "\n",
    "\"\"\"Compute K-F1++\"\"\"\n",
    "avg_kf1_score_content, _, avg_precision_score_content, _, avg_recall_score_content, _ = compute_avg_kf1_score(pipeline_data_control)\n",
    "append_to_file(results_file, f\"CONTROL-{what_to_ablate} K-F1++: {avg_kf1_score_content}, Precision: {avg_precision_score_content} Recall: {avg_recall_score_content}\")\n",
    "avg_kf1_score_content, _, avg_precision_score_content, _, avg_recall_score_content, _ = compute_avg_kf1_score(pipeline_data_main)\n",
    "append_to_file(results_file, f\"MAIN-{what_to_ablate} K-F1++: {avg_kf1_score_content}, Precision: {avg_precision_score_content} Recall: {avg_recall_score_content}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|▎         | 2/60 [00:07<03:16,  3.39s/it]WARNING:ragas.metrics._faithfulness:No statements were generated from the answer.\n",
      "Evaluating:  27%|██▋       | 16/60 [00:09<00:14,  3.00it/s]WARNING:ragas.metrics._faithfulness:No statements were generated from the answer.\n",
      "Evaluating:  33%|███▎      | 20/60 [00:10<00:13,  2.92it/s]WARNING:ragas.metrics._faithfulness:No statements were generated from the answer.\n",
      "Evaluating:  55%|█████▌    | 33/60 [00:28<00:30,  1.12s/it]WARNING:ragas.metrics._faithfulness:No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 60/60 [00:43<00:00,  1.37it/s]\n",
      "Evaluating:  40%|████      | 12/30 [00:01<00:01, 14.51it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.569000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.068000 seconds\n",
      "Evaluating:  50%|█████     | 15/30 [00:01<00:00, 16.61it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.096000 seconds\n",
      "Evaluating:  60%|██████    | 18/30 [00:01<00:00, 17.38it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.082000 seconds\n",
      "Evaluating:  83%|████████▎ | 25/30 [00:02<00:00,  8.96it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.116000 seconds\n",
      "Evaluating: 100%|██████████| 30/30 [00:04<00:00,  6.29it/s]\n",
      "Evaluating:  38%|███▊      | 19/50 [00:12<00:11,  2.66it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.491000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.243000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.451000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.320000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.280000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.985000 seconds\n",
      "Evaluating:  44%|████▍     | 22/50 [00:14<00:12,  2.24it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.763000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.520000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.007000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.122000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.825000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.032000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.386000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.949000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.930000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.947000 seconds\n",
      "Evaluating:  46%|████▌     | 23/50 [00:15<00:15,  1.69it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.613000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.297000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.513000 seconds\n",
      "Evaluating:  58%|█████▊    | 29/50 [00:16<00:06,  3.16it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.327000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.520000 seconds\n",
      "Evaluating:  60%|██████    | 30/50 [00:16<00:06,  3.03it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.018000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.068000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.229000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.218000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.985000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.121000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.287000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.624000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.334000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.461000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.083000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.134000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.909000 seconds\n",
      "Evaluating:  62%|██████▏   | 31/50 [00:18<00:09,  2.01it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.870000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.867000 seconds\n",
      "Evaluating:  64%|██████▍   | 32/50 [00:18<00:07,  2.30it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.557000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.457000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.025000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.045000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.122000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.011000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.984000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.990000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.005000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.966000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.777000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.531000 seconds\n",
      "Evaluating:  66%|██████▌   | 33/50 [00:20<00:12,  1.42it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.876000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.927000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.866000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.900000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.879000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.330000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.181000 seconds\n",
      "Evaluating:  68%|██████▊   | 34/50 [00:21<00:12,  1.27it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.248000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.345000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.208000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.210000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.180000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.865000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.003000 seconds\n",
      "Evaluating:  72%|███████▏  | 36/50 [00:22<00:09,  1.54it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.608000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.487000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.159000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.249000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.010000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.314000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.129000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.075000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.112000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.089000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.897000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.432000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.762000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.044000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.975000 seconds\n",
      "Evaluating:  74%|███████▍  | 37/50 [00:25<00:14,  1.11s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.788000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.590000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.328000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.282000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.108000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.060000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.109000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.969000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.062000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.959000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.474000 seconds\n",
      "Evaluating:  78%|███████▊  | 39/50 [00:27<00:11,  1.08s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.895000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.928000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.906000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.734000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.320000 seconds\n",
      "Evaluating:  80%|████████  | 40/50 [00:28<00:11,  1.13s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.950000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.935000 seconds\n",
      "Evaluating:  82%|████████▏ | 41/50 [00:28<00:08,  1.11it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.938000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.070000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.128000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.907000 seconds\n",
      "Evaluating:  84%|████████▍ | 42/50 [00:31<00:10,  1.36s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.101000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.763000 seconds\n",
      "Evaluating:  88%|████████▊ | 44/50 [00:32<00:06,  1.10s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.253000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.965000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.592000 seconds\n",
      "Evaluating:  90%|█████████ | 45/50 [00:33<00:05,  1.12s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.231000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.198000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.233000 seconds\n",
      "Evaluating: 100%|██████████| 50/50 [00:50<00:00,  1.00s/it]\n",
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.384000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.431000 seconds\n",
      "Evaluating:  12%|█▏        | 3/25 [00:00<00:05,  4.17it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.035000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.047000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.054000 seconds\n",
      "Evaluating:  16%|█▌        | 4/25 [00:01<00:05,  4.16it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.501000 seconds\n",
      "Evaluating:  20%|██        | 5/25 [00:01<00:04,  4.34it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.067000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.428000 seconds\n",
      "Evaluating:  24%|██▍       | 6/25 [00:01<00:05,  3.78it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.228000 seconds\n",
      "Evaluating:  28%|██▊       | 7/25 [00:01<00:04,  3.63it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.069000 seconds\n",
      "Evaluating:  32%|███▏      | 8/25 [00:02<00:04,  3.99it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.810000 seconds\n",
      "Evaluating:  36%|███▌      | 9/25 [00:02<00:03,  4.87it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.403000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.527000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.287000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.213000 seconds\n",
      "Evaluating:  48%|████▊     | 12/25 [00:02<00:02,  5.01it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.346000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.020000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.108000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.531000 seconds\n",
      "Evaluating:  56%|█████▌    | 14/25 [00:03<00:02,  4.09it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.285000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.726000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.471000 seconds\n",
      "Evaluating:  72%|███████▏  | 18/25 [00:04<00:01,  4.00it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.048000 seconds\n",
      "Evaluating:  88%|████████▊ | 22/25 [00:05<00:00,  3.60it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.435000 seconds\n",
      "Evaluating: 100%|██████████| 25/25 [00:09<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleepy time...\n",
      "Okay I'm back now...\n",
      "i=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|████▍     | 22/50 [00:14<00:14,  1.88it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.061000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.122000 seconds\n",
      "Evaluating:  56%|█████▌    | 28/50 [00:15<00:08,  2.65it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.663000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.384000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.457000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.621000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.044000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.281000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.199000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.152000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.988000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.671000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.984000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.977000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.561000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.969000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.279000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.488000 seconds\n",
      "Evaluating:  62%|██████▏   | 31/50 [00:17<00:08,  2.29it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.265000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.285000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.988000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.872000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.852000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.774000 seconds\n",
      "Evaluating:  68%|██████▊   | 34/50 [00:18<00:06,  2.52it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.451000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.414000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.391000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.051000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.273000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.288000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.257000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.304000 seconds\n",
      "Evaluating:  70%|███████   | 35/50 [00:19<00:06,  2.22it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.304000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.319000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.165000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.073000 seconds\n",
      "Evaluating:  72%|███████▏  | 36/50 [00:19<00:06,  2.29it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.604000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.121000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.050000 seconds\n",
      "Evaluating:  74%|███████▍  | 37/50 [00:20<00:06,  1.94it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.491000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.056000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.276000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.169000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.089000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.045000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.947000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.910000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.845000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.773000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.651000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.596000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.612000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.642000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.015000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.981000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.070000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.025000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.005000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.013000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.003000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.000000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.973000 seconds\n",
      "Evaluating:  76%|███████▌  | 38/50 [00:23<00:10,  1.17it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.479000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.246000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.249000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.153000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.023000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.215000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.089000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.879000 seconds\n",
      "Evaluating:  78%|███████▊  | 39/50 [00:25<00:13,  1.19s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.848000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.612000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.365000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.447000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.276000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.107000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.122000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.158000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.143000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.119000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.864000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.252000 seconds\n",
      "Evaluating:  80%|████████  | 40/50 [00:28<00:15,  1.51s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.655000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.545000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.330000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.268000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.182000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.257000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.612000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.555000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.594000 seconds\n",
      "Evaluating:  84%|████████▍ | 42/50 [00:30<00:10,  1.25s/it]INFO:openai._base_client:Retrying request to /chat/completions in 1.239000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.242000 seconds\n",
      "Evaluating: 100%|██████████| 50/50 [00:47<00:00,  1.04it/s]\n",
      "Evaluating:   0%|          | 0/25 [00:00<?, ?it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.302000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.051000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.026000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.045000 seconds\n",
      "Evaluating:   4%|▍         | 1/25 [00:00<00:13,  1.72it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.210000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.190000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.198000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.181000 seconds\n",
      "Evaluating:   8%|▊         | 2/25 [00:01<00:11,  2.03it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.368000 seconds\n",
      "Evaluating:  20%|██        | 5/25 [00:01<00:04,  4.73it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.212000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.072000 seconds\n",
      "Evaluating:  24%|██▍       | 6/25 [00:01<00:03,  5.29it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.426000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.486000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.354000 seconds\n",
      "Evaluating:  28%|██▊       | 7/25 [00:01<00:03,  4.62it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.158000 seconds\n",
      "Evaluating:  36%|███▌      | 9/25 [00:02<00:02,  5.97it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.612000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.572000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.364000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.276000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.115000 seconds\n",
      "Evaluating:  44%|████▍     | 11/25 [00:02<00:02,  6.08it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.084000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.370000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.341000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.384000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.206000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.362000 seconds\n",
      "Evaluating:  48%|████▊     | 12/25 [00:03<00:03,  3.64it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.163000 seconds\n",
      "Evaluating:  60%|██████    | 15/25 [00:03<00:01,  5.13it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.129000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.346000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.715000 seconds\n",
      "Evaluating:  68%|██████▊   | 17/25 [00:04<00:02,  3.17it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.384000 seconds\n",
      "Evaluating:  72%|███████▏  | 18/25 [00:05<00:02,  2.55it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.537000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.333000 seconds\n",
      "Evaluating:  76%|███████▌  | 19/25 [00:05<00:02,  2.56it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.432000 seconds\n",
      "Evaluating:  84%|████████▍ | 21/25 [00:06<00:01,  2.24it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.367000 seconds\n",
      "Evaluating: 100%|██████████| 25/25 [00:12<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleepy time...\n",
      "Okay I'm back now...\n",
      "i=25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  36%|███▌      | 18/50 [00:15<00:26,  1.22it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.726000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.624000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.033000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.806000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.817000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.225000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.746000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.756000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.751000 seconds\n",
      "Evaluating:  58%|█████▊    | 29/50 [00:17<00:08,  2.48it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.142000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.210000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.184000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.368000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.249000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.241000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.654000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.101000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.017000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.459000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.235000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.122000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.684000 seconds\n",
      "Evaluating:  66%|██████▌   | 33/50 [00:19<00:07,  2.40it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.132000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.756000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.926000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.732000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.741000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.197000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.999000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.314000 seconds\n",
      "Evaluating:  74%|███████▍  | 37/50 [00:21<00:05,  2.24it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.072000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.478000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.971000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.041000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.897000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.915000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.828000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.846000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.467000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.195000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.053000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.030000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.191000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.826000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.032000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.228000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.962000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.967000 seconds\n",
      "Evaluating:  76%|███████▌  | 38/50 [00:24<00:08,  1.49it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.941000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.747000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.451000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.875000 seconds\n",
      "Evaluating:  78%|███████▊  | 39/50 [00:25<00:07,  1.44it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.252000 seconds\n",
      "Evaluating:  80%|████████  | 40/50 [00:25<00:06,  1.57it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.305000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.296000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.882000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.301000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.822000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.234000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.212000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.053000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.966000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.658000 seconds\n",
      "Evaluating:  82%|████████▏ | 41/50 [00:28<00:08,  1.04it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.432000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.376000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.999000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.371000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.537000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.217000 seconds\n",
      "Evaluating:  88%|████████▊ | 44/50 [00:30<00:05,  1.17it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.050000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.272000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.211000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.959000 seconds\n",
      "Evaluating:  90%|█████████ | 45/50 [00:35<00:08,  1.71s/it]INFO:openai._base_client:Retrying request to /chat/completions in 0.933000 seconds\n",
      "Evaluating: 100%|██████████| 50/50 [00:57<00:00,  1.15s/it]\n",
      "Evaluating: 100%|██████████| 25/25 [00:09<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleepy time...\n",
      "Okay I'm back now...\n",
      "i=50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/50 [00:00<?, ?it/s]ERROR:asyncio:Task exception was never retrieved\n",
      "future: <Task finished name='Task-6365' coro=<AsyncClient.aclose() done, defined at /Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpx/_client.py:2011> exception=RuntimeError('Event loop is closed')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpx/_client.py\", line 2018, in aclose\n",
      "    await self._transport.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpx/_transports/default.py\", line 385, in aclose\n",
      "    await self._pool.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 313, in aclose\n",
      "    await self._close_connections(closing_connections)\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py\", line 305, in _close_connections\n",
      "    await connection.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py\", line 171, in aclose\n",
      "    await self._connection.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpcore/_async/http11.py\", line 265, in aclose\n",
      "    await self._network_stream.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py\", line 54, in aclose\n",
      "    await self._stream.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/anyio/streams/tls.py\", line 202, in aclose\n",
      "    await self.transport_stream.aclose()\n",
      "  File \"/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 1191, in aclose\n",
      "    self._transport.close()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py\", line 860, in close\n",
      "    self._loop.call_soon(self._call_connection_lost, None)\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 761, in call_soon\n",
      "    self._check_closed()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 519, in _check_closed\n",
      "    raise RuntimeError('Event loop is closed')\n",
      "RuntimeError: Event loop is closed\n",
      "Evaluating:  54%|█████▍    | 27/50 [00:18<00:11,  2.07it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.478000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.891000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.035000 seconds\n",
      "Evaluating:  62%|██████▏   | 31/50 [00:21<00:10,  1.84it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.343000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.095000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.232000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.233000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.879000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.321000 seconds\n",
      "Evaluating:  76%|███████▌  | 38/50 [00:22<00:04,  2.70it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.168000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.643000 seconds\n",
      "Evaluating:  78%|███████▊  | 39/50 [00:23<00:03,  2.80it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.105000 seconds\n",
      "Evaluating:  80%|████████  | 40/50 [00:23<00:03,  2.95it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.250000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.178000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.490000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.239000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.850000 seconds\n",
      "Evaluating:  82%|████████▏ | 41/50 [00:24<00:03,  2.51it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.731000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.734000 seconds\n",
      "Evaluating:  86%|████████▌ | 43/50 [00:25<00:03,  2.29it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.601000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.498000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.149000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.019000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.969000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 1.173000 seconds\n",
      "Evaluating:  88%|████████▊ | 44/50 [00:27<00:04,  1.40it/s]INFO:openai._base_client:Retrying request to /chat/completions in 1.270000 seconds\n",
      "Evaluating:  94%|█████████▍| 47/50 [00:28<00:01,  1.69it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.976000 seconds\n",
      "Evaluating: 100%|██████████| 50/50 [00:46<00:00,  1.07it/s]\n",
      "Evaluating:  24%|██▍       | 6/25 [00:01<00:02,  6.37it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.135000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.107000 seconds\n",
      "Evaluating:  52%|█████▏    | 13/25 [00:01<00:01, 11.37it/s]INFO:openai._base_client:Retrying request to /chat/completions in 0.136000 seconds\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.148000 seconds\n",
      "Evaluating: 100%|██████████| 25/25 [00:12<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from evals.automatic.ragas import run_for_pipeline_5_7, compute_average_metrics_from_csv\n",
    "from evals.automatic.test_utils import extract_eval_data\n",
    "from evals.automatic.test_utils import append_to_file\n",
    "import pandas as pd\n",
    "\n",
    "################################\n",
    "what_to_ablate = Literal[\"query-rewriter\", \"sparse-retriever\", \"reranker\", \"sample-questions\", \"one-shot\"] \n",
    "\n",
    "# What do you wish to exclude?\n",
    "what_to_ablate = \"one-shot\"\n",
    "################################\n",
    "\n",
    "# This is where we will store all results\n",
    "results_file = os.path.join(\"..\", \"..\", \"evals\", \"ablation\", \"results\", \"results.txt\")\n",
    "\n",
    "if what_to_ablate == \"query-rewriter\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite-control.json\")\n",
    "elif what_to_ablate == \"sparse-retriever\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever-control.json\")\n",
    "elif what_to_ablate == \"reranker\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker-control.json\")\n",
    "elif what_to_ablate == \"sample-questions\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions-control.json\")\n",
    "elif what_to_ablate == \"one-shot\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot-control.json\")\n",
    "else: \n",
    "    raise Exception(\"You haven't specified a correct ablation to evaluate.\")\n",
    "\n",
    "# Extract the data to PipelineData format\n",
    "pipeline_data_main = extract_eval_data(data_file_main)\n",
    "pipeline_data_main = pipeline_data_main # 300 is too expensive to compute for RAGAS so just do 100?\n",
    "pipeline_data_control = extract_eval_data(data_file_control)\n",
    "\n",
    "\n",
    "import time\n",
    "def process_in_batches(data, batch_size, wait_time, get_context_relevance):\n",
    "    total_elements = len(data)\n",
    "    for i in range(0, total_elements, batch_size):\n",
    "        batch = data[i:i + batch_size]\n",
    "        df_main_partial = run_for_pipeline_5_7(batch, get_context_relevance=get_context_relevance)\n",
    "        \n",
    "        yield df_main_partial\n",
    "        \n",
    "        if i + batch_size < total_elements:\n",
    "            print(\"Sleepy time...\")\n",
    "            time.sleep(wait_time)\n",
    "            print(\"Okay I'm back now...\")\n",
    "            print(f\"i={i}\")\n",
    "\n",
    "\"\"\"Compute RAGAS metrics\"\"\"\n",
    "if what_to_ablate not in [\"sample-questions\", \"one-shot\"]:\n",
    "    # compute G, AR, CR, and RCR\n",
    "    columns_to_average = ['answer_relevancy', 'faithfulness', 'context_relevancy', 'reverse_context_relevancy']  # Specify the columns you want to average\n",
    "\n",
    "    df_control = run_for_pipeline_5_7(pipeline_data_control, get_context_relevance=True)\n",
    "    average_gr, average_ar, average_cr, average_rcr = compute_average_metrics_from_csv(df_control, columns_to_average)\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} RAGAS Groundedness: {average_gr}, Answer Relevancy: {average_ar}, Context Relevancy: {average_cr}, Reverse Context Relevancy: {average_rcr}\")\n",
    "\n",
    "    time.sleep(30)\n",
    "    \n",
    "    partial_main_dfs = []\n",
    "    for df_partial in process_in_batches(pipeline_data_main, 25, 30, get_context_relevance=True):\n",
    "        partial_main_dfs.append(df_partial)\n",
    "    \n",
    "    # Concatenate all the partial dataframes into one big dataframe\n",
    "    df_main = pd.concat(partial_main_dfs, ignore_index=True)\n",
    "\n",
    "    # df_main = run_for_pipeline_5_7(pipeline_data_main, get_context_relevance=True)\n",
    "    average_gr, average_ar, average_cr, average_rcr = compute_average_metrics_from_csv(df_main, columns_to_average)\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} RAGAS Groundedness: {average_gr}, Answer Relevancy: {average_ar}, Context Relevancy: {average_cr}, Reverse Context Relevancy: {average_rcr}\")\n",
    "\n",
    "else:\n",
    "    # compute G, AR, RCR\n",
    "    columns_to_average = ['answer_relevancy', 'faithfulness', 'reverse_context_relevancy']  # Specify the columns you want to average\n",
    "\n",
    "    df_control = run_for_pipeline_5_7(pipeline_data_control, get_context_relevance=False)\n",
    "    average_gr, average_ar, average_rcr = compute_average_metrics_from_csv(df_control, columns_to_average)\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} RAGAS Groundedness: {average_gr}, Answer Relevancy: {average_ar}, Reverse Context Relevancy: {average_rcr}\")\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "    partial_main_dfs = []\n",
    "    for df_partial in process_in_batches(pipeline_data_main, 25, 30, get_context_relevance=False):\n",
    "        partial_main_dfs.append(df_partial)\n",
    "    \n",
    "    # Concatenate all the partial dataframes into one big dataframe\n",
    "    df_main = pd.concat(partial_main_dfs, ignore_index=True)\n",
    "\n",
    "    # df_main = run_for_pipeline_5_7(pipeline_data_main, get_context_relevance=True)\n",
    "    average_gr, average_ar, average_rcr = compute_average_metrics_from_csv(df_main, columns_to_average)\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} RAGAS Groundedness: {average_gr}, Answer Relevancy: {average_ar}, Reverse Context Relevancy: {average_rcr}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
