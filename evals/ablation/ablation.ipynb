{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study Calculations\n",
    "This notebook contains all my code to run the ablation study and to compute the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from typing import Literal\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "DATA_DIR = os.getenv(\"DATA_DIR_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victoroldensand/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "notebook_dir = os.getcwd() # Get the current working directory of the notebook\n",
    "src_dir = os.path.abspath(os.path.join(notebook_dir, '..', '..')) # Construct the path to the src directory\n",
    "sys.path.append(src_dir) # Add the src directory to the system path\n",
    "\n",
    "from src.pipelines.pipeline_runner import run_data_through_generator, run_data_through_generator_threaded\n",
    "from src.utils import load_json_to_pipelinedata, save_objects_as_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete the ablation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  3.95it/s]0:00<?, ?it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  7.12it/s]0:16<07:58, 16.50s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.02it/s]0:27<06:06, 13.10s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.50it/s]0:29<03:36,  8.02s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.91it/s]0:32<02:40,  6.16s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 20.79it/s]0:35<02:04,  4.97s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.84it/s]0:36<01:30,  3.76s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 16.19it/s]0:39<01:15,  3.29s/it]\n",
      "Running through generator...:  27%|██▋       | 8/30 [00:42<01:13,  3.36s/it]INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  5.64it/s]\n",
      "Running through generator...:  30%|███       | 9/30 [00:53<01:59,  5.70s/it]INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 2.000000 seconds\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  4.33it/s]\n",
      "Running through generator...:  33%|███▎      | 10/30 [01:04<02:25,  7.28s/it]INFO:groq._base_client:Retrying request to /openai/v1/chat/completions in 4.000000 seconds\n",
      "Running through generator...:  33%|███▎      | 10/30 [01:09<02:18,  6.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/_base_client.py:966\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/httpx/_models.py:761\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[0;32m--> 761\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m             time\u001b[38;5;241m.\u001b[39msleep(wait_time)\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOkay I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm back now...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m \u001b[43mprocess_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincomplete_pipeline_data_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file_main\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllama3-70b-8192\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mablation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# res = run_data_through_generator(incomplete_pipeline_data_main, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# save_objects_as_json(res, output_file_main, rewrite=True)\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# restart from i = 4 I believe\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mprocess_in_batches\u001b[0;34m(data, batch_size, wait_time, output_file, model, ablation_params, verbose)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     72\u001b[0m batch \u001b[38;5;241m=\u001b[39m data[i:i \u001b[38;5;241m+\u001b[39m batch_size]\n\u001b[0;32m---> 73\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mrun_data_through_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mablation_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mablation_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m save_objects_as_json(res, output_file, rewrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m batch_size \u001b[38;5;241m<\u001b[39m total_elements:\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/src/pipelines/pipeline_runner.py:263\u001b[0m, in \u001b[0;36mrun_data_through_generator\u001b[0;34m(pipe_data, model, ablation_params, verbose)\u001b[0m\n\u001b[1;32m    254\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    255\u001b[0m     PIPELINE_QUESTION_GENERATOR_PROMPT\u001b[38;5;241m.\u001b[39mformat()\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_one_shot\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m PIPELINE_QUESTION_GENERATOR_PROMPT_ZERO_SHOT\u001b[38;5;241m.\u001b[39mformat()\n\u001b[1;32m    258\u001b[0m )\n\u001b[1;32m    259\u001b[0m user_prompt \u001b[38;5;241m=\u001b[39m PIPELINE_QUESTION_GENERATOR_USER_PROMPT\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    260\u001b[0m     query\u001b[38;5;241m=\u001b[39mitem\u001b[38;5;241m.\u001b[39mquery\u001b[38;5;241m.\u001b[39mquery, context_str\u001b[38;5;241m=\u001b[39mcontext\n\u001b[1;32m    261\u001b[0m )\n\u001b[0;32m--> 263\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m res_proper: ResponseSchema \u001b[38;5;241m=\u001b[39m ResponseSchema(\n\u001b[1;32m    266\u001b[0m     text\u001b[38;5;241m=\u001b[39mres, embedding\u001b[38;5;241m=\u001b[39mget_embedding(res, embedding_model)\n\u001b[1;32m    267\u001b[0m )\n\u001b[1;32m    269\u001b[0m pip_data: PipelineData \u001b[38;5;241m=\u001b[39m PipelineData(\n\u001b[1;32m    270\u001b[0m     query\u001b[38;5;241m=\u001b[39mitem\u001b[38;5;241m.\u001b[39mquery, retrieved_docs\u001b[38;5;241m=\u001b[39mretrieved_docs, response\u001b[38;5;241m=\u001b[39mres_proper\n\u001b[1;32m    271\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/src/pipelines/pipeline_runner.py:147\u001b[0m, in \u001b[0;36mpipeline_generator\u001b[0;34m(prompt, query, model, verbose)\u001b[0m\n\u001b[1;32m    144\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser prompt: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-70b-8192\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 147\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgroq_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     res \u001b[38;5;241m=\u001b[39m openai_request(\n\u001b[1;32m    155\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    156\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m         messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[1;32m    158\u001b[0m         max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Adjust based on the expected length of the enhanced query\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/backoff/_sync.py:105\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    102\u001b[0m }\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    107\u001b[0m     max_tries_exceeded \u001b[38;5;241m=\u001b[39m (tries \u001b[38;5;241m==\u001b[39m max_tries_value)\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/src/llms/groq_requests.py:45\u001b[0m, in \u001b[0;36mgroq_request\u001b[0;34m(llm, verbose, **params)\u001b[0m\n\u001b[1;32m     39\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of OpenAI-equivalent tokens in the payload:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_tokens_from_messages(messages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     full_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: llm, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams}\n\u001b[0;32m---> 45\u001b[0m     completion \u001b[38;5;241m=\u001b[39m \u001b[43msync_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfull_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m groq\u001b[38;5;241m.\u001b[39mRateLimitError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Log and re-raise rate limit errors\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/resources/chat/completions.py:178\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    162\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    163\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    Creates a completion for a chat prompt\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/_base_client.py:1194\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1182\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1191\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1192\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1193\u001b[0m     )\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/_base_client.py:896\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    888\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    889\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    894\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    895\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/_base_client.py:972\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m    971\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m--> 972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m~/Documents/KTH/master-thesis/codebase/twiga/.venv/lib/python3.11/site-packages/groq/_base_client.py:1018\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1014\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, options\u001b[38;5;241m.\u001b[39murl, timeout)\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m-> 1018\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m   1021\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   1022\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m   1026\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "what_to_ablate = Literal[\"query-rewriter\", \"sparse-retriever\", \"reranker\", \"sample-questions\", \"one-shot\"] \n",
    "\n",
    "################################\n",
    "# What do you wish to get rid of in this run?\n",
    "what_to_ablate = \"query-rewriter\"\n",
    "################################\n",
    "\n",
    "ablation_params = {\n",
    "        \"no_rewriter\": False,\n",
    "        \"no_sparse_retriever\": False,\n",
    "        \"no_reranker\": False,\n",
    "        \"no_sample_questions\": False,\n",
    "        \"no_one_shot\": False\n",
    "    }\n",
    "\n",
    "if what_to_ablate == \"query-rewriter\":\n",
    "    input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-query-rewrite).json\")\n",
    "    output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite.json\")\n",
    "    input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-query-rewrite).json\")\n",
    "    output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite-control.json\")\n",
    "    ablation_params[\"no_rewriter\"] = True\n",
    "elif what_to_ablate == \"sparse-retriever\":\n",
    "    input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-sparse-retrieval).json\")\n",
    "    output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever.json\")\n",
    "    input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-sparse-retrieval).json\")\n",
    "    output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever-control.json\")\n",
    "    ablation_params[\"no_sparse_retriever\"] = True\n",
    "elif what_to_ablate == \"reranker\":\n",
    "    input_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"retrieved-(no-query-rewrite).json\")\n",
    "    output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker.json\")\n",
    "    input_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"datasets\", \"control-retrieved-(no-query-rewrite).json\")\n",
    "    output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker-control.json\")\n",
    "    ablation_params[\"no_reranker\"] = True\n",
    "elif what_to_ablate == \"sample-questions\":\n",
    "    input_file_main = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"test-prompts-rewritten-retrieved.json\")\n",
    "    output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions.json\")\n",
    "    input_file_control = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"control-test-prompts-rewritten-retrieved.json\")\n",
    "    output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions-control.json\")\n",
    "    ablation_params[\"no_sample_questions\"] = True\n",
    "elif what_to_ablate == \"one-shot\":\n",
    "    input_file_main = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"test-prompts-rewritten-retrieved.json\")\n",
    "    output_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot.json\")\n",
    "    input_file_control = os.path.join(\"..\", \"..\", \"data\", \"main\", \"datasets\", \"control-test-prompts-rewritten-retrieved.json\")\n",
    "    output_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot-control.json\")\n",
    "    ablation_params[\"no_one_shot\"] = True\n",
    "else: \n",
    "    raise Exception(\"You haven't specified a correct ablation to generate.\")\n",
    "\n",
    "with open(input_file_control, 'r') as file:\n",
    "    data_control = json.load(file)\n",
    "    incomplete_pipeline_data_control = load_json_to_pipelinedata(data_control)\n",
    "\n",
    "with open(input_file_main, 'r') as file:\n",
    "    data_main = json.load(file)\n",
    "    incomplete_pipeline_data_main = load_json_to_pipelinedata(data_main)\n",
    "\n",
    "# # Run the ablation study using llama3-70b-8192 due to its success in the \n",
    "# res = run_data_through_generator(incomplete_pipeline_data_control, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\n",
    "# save_objects_as_json(res, output_file_control, rewrite=True)\n",
    "\n",
    "def process_in_batches(data, batch_size, wait_time, output_file, model, ablation_params, verbose=False):\n",
    "    total_elements = len(data)\n",
    "    for i in range(0, total_elements, batch_size):\n",
    "        # THIS IS TEMPORARY\n",
    "        if i ==0:\n",
    "            continue\n",
    "        if i ==1:\n",
    "            continue\n",
    "        if i ==2:\n",
    "            continue\n",
    "\n",
    "        batch = data[i:i + batch_size]\n",
    "        res = run_data_through_generator(batch, model, ablation_params=ablation_params, verbose=verbose)\n",
    "        save_objects_as_json(res, output_file, rewrite=False)\n",
    "        if i + batch_size < total_elements:\n",
    "            print(\"Sleepy time...\")\n",
    "            time.sleep(wait_time)\n",
    "            print(\"Okay I'm back now...\")\n",
    "\n",
    "process_in_batches(incomplete_pipeline_data_main, 30, 5, output_file_main, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\n",
    "\n",
    "# res = run_data_through_generator(incomplete_pipeline_data_main, \"llama3-70b-8192\", ablation_params=ablation_params, verbose=False)\n",
    "# save_objects_as_json(res, output_file_main, rewrite=True)\n",
    "\n",
    "# restart from i = 4 I believe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation study automatic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evals.automatic.test_utils import extract_eval_data\n",
    "from evals.automatic.hit_ratio_and_mrr import compute_hit_ratio_and_mrr\n",
    "from evals.automatic.test_utils import append_to_file\n",
    "\n",
    "################################\n",
    "# What do you wish to get rid of in this run?\n",
    "what_to_ablate = \"query-rewriter\"\n",
    "################################\n",
    "\n",
    "# This is where we will store all results\n",
    "results_file = os.path.join(\"..\", \"..\", \"evals\", \"ablation\", \"results\", \"results.txt\")\n",
    "\n",
    "if what_to_ablate == \"query-rewriter\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-query-rewrite-control.json\")\n",
    "elif what_to_ablate == \"sparse-retriever\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sparse-retriever-control.json\")\n",
    "elif what_to_ablate == \"reranker\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-reranker-control.json\")\n",
    "elif what_to_ablate == \"sample-questions\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-sample-questions-control.json\")\n",
    "elif what_to_ablate == \"one-shot\":\n",
    "    data_file_main = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot.json\")\n",
    "    data_file_control = os.path.join(\"..\", \"..\", \"data\", \"ablation\", \"complete_runs\", \"ablation-llama3-no-one-shot-control.json\")\n",
    "else: \n",
    "    raise Exception(\"You haven't specified a correct ablation to evaluate.\")\n",
    "\n",
    "# Extract the data to PipelineData format\n",
    "pipeline_data_main = extract_eval_data(data_file_main)\n",
    "pipeline_data_control = extract_eval_data(data_file_control)\n",
    "\n",
    "\"\"\"Compute the Hit Rate and MRR\"\"\"\n",
    "\n",
    "if what_to_ablate not in [\"sample-questions\", \"one-shot\"]:\n",
    "    # For control\n",
    "    _, hit_rate, mrr = compute_hit_ratio_and_mrr(pipeline_data_control)\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} Hit Rate: {hit_rate}\")\n",
    "    append_to_file(results_file, f\"CONTROL-{what_to_ablate} MRR: {mrr}\")\n",
    "\n",
    "    # For main\n",
    "    _, hit_rate, mrr = compute_hit_ratio_and_mrr(pipeline_data_main)\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} Hit Rate: {hit_rate}\")\n",
    "    append_to_file(results_file, f\"MAIN-{what_to_ablate} MRR: {mrr}\")\n",
    "\n",
    "\"\"\"Compute BERTScore\"\"\"\n",
    "\n",
    "\"\"\"Compute Embedding Similarity\"\"\"\n",
    "\n",
    "\"\"\"Compute K-F1++\"\"\"\n",
    "\n",
    "\"\"\"Compute RAGAS metrics\"\"\"\n",
    "if what_to_ablate not in [\"sample-questions\", \"one-shot\"]:\n",
    "    # compute G, AR, CR, and RCR\n",
    "    pass\n",
    "else:\n",
    "    # compute G, AR, RCR\n",
    "    pass\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
